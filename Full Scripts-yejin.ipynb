{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8eb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from time import time\n",
    "from tqdm import tqdm   # ⬅ tqdm 추가\n",
    "\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.policy import LinUCB, LinTS\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation,\n",
    "    InverseProbabilityWeighting,\n",
    "    DoublyRobust,\n",
    "    SwitchDoublyRobust,\n",
    "    SubGaussianDoublyRobust,\n",
    "    DoublyRobustTuning,\n",
    "    RegressionModel,\n",
    ")\n",
    "\n",
    "#############################################################\n",
    "# 1) Load Open Bandit Dataset\n",
    "#############################################################\n",
    "\n",
    "def load_obd():\n",
    "    dataset = OpenBanditDataset(\n",
    "        behavior_policy=\"random\",\n",
    "        campaign=\"all\",\n",
    "        data_path=\"C:\\\\Users\\\\lyms0\\\\Downloads\\\\open_bandit_dataset\\\\open_bandit_dataset\"\n",
    "    )\n",
    "    fb = dataset.obtain_batch_bandit_feedback()\n",
    "    print(f\"[INFO] Loaded rounds={fb['n_rounds']} actions={fb['n_actions']}\")\n",
    "    return dataset, fb\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# 2) Train LinUCB / LinTS using logged data (with tqdm)\n",
    "#############################################################\n",
    "\n",
    "def train_policy(policy, fb):\n",
    "    X, A, R = fb[\"context\"], fb[\"action\"], fb[\"reward\"]\n",
    "\n",
    "    print(f\"[TRAIN] Training {policy.policy_name} ...\")\n",
    "    for x, a, r in tqdm(zip(X, A, R), total=len(R)):\n",
    "        policy.update_params(\n",
    "            action=int(a),\n",
    "            reward=float(r),\n",
    "            context=x.reshape(1, -1),\n",
    "        )\n",
    "    return policy\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# 3) deterministic action_dist (with tqdm)\n",
    "#############################################################\n",
    "\n",
    "def compute_action_dist(policy, X, n_actions):\n",
    "    n = X.shape[0]\n",
    "    dist = np.zeros((n, n_actions, 1))\n",
    "\n",
    "    print(f\"[ACTION_DIST] Computing action_dist for {policy.policy_name} ...\")\n",
    "    for i in tqdm(range(n)):\n",
    "        action = policy.select_action(X[i].reshape(1, -1))[0]\n",
    "        dist[i, action, 0] = 1.0\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# 4) Reward model for DR, Switch-DR, SG-DR, MRDR\n",
    "#############################################################\n",
    "\n",
    "def build_reward_model(fb, n_actions):\n",
    "    print(\"[REWARD MODEL] Fitting logistic regression models ...\")\n",
    "    start = time()\n",
    "\n",
    "    model = RegressionModel(\n",
    "        n_actions=n_actions,\n",
    "        len_list=1,\n",
    "        base_model=LogisticRegression(max_iter=500),\n",
    "    )\n",
    "    pred = model.fit_predict(fb)\n",
    "\n",
    "    print(f\"[DONE] Reward model training took {time() - start:.2f} sec\\n\")\n",
    "    return pred\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# 5) OPE\n",
    "#############################################################\n",
    "\n",
    "def run_ope(fb, action_dist, reward_model):\n",
    "    ope = OffPolicyEvaluation(\n",
    "        fb,\n",
    "        ope_estimators=[\n",
    "            InverseProbabilityWeighting(),\n",
    "            DoublyRobust(),\n",
    "            SwitchDoublyRobust(),\n",
    "            SubGaussianDoublyRobust(),\n",
    "            DoublyRobustTuning(\n",
    "                lambdas=[0.0, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "                tuning_method=\"slope\",\n",
    "                estimator_name=\"mrdr\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(\"[OPE] Estimating policy values ...\")\n",
    "    start = time()\n",
    "\n",
    "    values = ope.estimate_policy_values(\n",
    "        action_dist=action_dist,\n",
    "        estimated_rewards_by_reg_model=reward_model,\n",
    "    )\n",
    "    intervals = ope.estimate_interval(\n",
    "        action_dist=action_dist,\n",
    "        estimated_rewards_by_reg_model=reward_model,\n",
    "        alpha=0.05,\n",
    "    )\n",
    "\n",
    "    print(f\"[DONE] OPE took {time() - start:.2f} sec\\n\")\n",
    "    return values, intervals\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# 6) Full Experiment Runner\n",
    "#############################################################\n",
    "\n",
    "def run_experiment():\n",
    "    total_start = time()   # 전체 시간 측정 시작\n",
    "\n",
    "    dataset, fb = load_obd()\n",
    "    X, n_actions = fb[\"context\"], fb[\"n_actions\"]\n",
    "\n",
    "    baseline = fb[\"reward\"].mean()\n",
    "    print(f\"\\n[Random Baseline Reward] {baseline:.4f}\\n\")\n",
    "\n",
    "    # Train LinUCB and LinTS\n",
    "    t0 = time()\n",
    "    linucb = train_policy(LinUCB(dim=X.shape[1], n_actions=n_actions, len_list=1), fb)\n",
    "    print(f\"[TIME] LinUCB training took {time() - t0:.2f} sec\\n\")\n",
    "\n",
    "    t1 = time()\n",
    "    lints  = train_policy(LinTS(dim=X.shape[1], n_actions=n_actions, len_list=1), fb)\n",
    "    print(f\"[TIME] LinTS training took {time() - t1:.2f} sec\\n\")\n",
    "\n",
    "    # Compute action distributions\n",
    "    t2 = time()\n",
    "    dist_ucb = compute_action_dist(linucb, X, n_actions)\n",
    "    print(f\"[TIME] LinUCB action_dist took {time() - t2:.2f} sec\\n\")\n",
    "\n",
    "    t3 = time()\n",
    "    dist_ts  = compute_action_dist(lints, X, n_actions)\n",
    "    print(f\"[TIME] LinTS action_dist took {time() - t3:.2f} sec\\n\")\n",
    "\n",
    "    # Reward model\n",
    "    reward_model = build_reward_model(fb, n_actions)\n",
    "\n",
    "    # OPE\n",
    "    v_ucb, ci_ucb = run_ope(fb, dist_ucb, reward_model)\n",
    "    v_ts, ci_ts   = run_ope(fb, dist_ts,  reward_model)\n",
    "\n",
    "    print(\"\\n========= LinUCB =========\")\n",
    "    for name, v in v_ucb.items():\n",
    "        print(f\"{name:<25} {v:.4f}   CI=({ci_ucb[name]['lower']:.4f}, {ci_ucb[name]['upper']:.4f})\")\n",
    "\n",
    "    print(\"\\n========= LinTS =========\")\n",
    "    for name, v in v_ts.items():\n",
    "        print(f\"{name:<25} {v:.4f}   CI=({ci_ts[name]['lower']:.4f}, {ci_ts[name]['upper']:.4f})\")\n",
    "\n",
    "    print(f\"\\n[TOTAL TIME] Entire experiment took {time() - total_start:.2f} sec\")\n",
    "\n",
    "\n",
    "run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandit310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
